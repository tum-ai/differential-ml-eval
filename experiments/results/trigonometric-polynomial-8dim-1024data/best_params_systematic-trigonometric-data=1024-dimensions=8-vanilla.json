{"lr_dml": 0.0002205039724956876, "n_layers": 4, "hidden_layer_sizes": 64, "batch_size": 1024, "epochs": 1000, "activation": "relu", "regularization_scale": 0}